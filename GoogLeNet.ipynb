{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrJHSIM/DeepLearningArticles/blob/main/GoogLeNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GoogLeNet 코드 reference : https://deep-learning-study.tistory.com/523"
      ],
      "metadata": {
        "id": "2ULhfV0puNtH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQG81-dJa5KT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "wpfq6bAnD3H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X2EVh9Wccw8",
        "outputId": "e83e1ba3-696b-4013-8930-7c0b0d8502d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12380433.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform = transform)\n",
        "\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform = transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8m1Lwpvd3cA"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LupnOzj1d5Vv",
        "outputId": "955cc297-87e1-4f7c-ffec-595928659178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self,aux_logits=True, num_classes=10):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        assert aux_logits == True or aux_logits == False\n",
        "        self.aux_logits = aux_logits\n",
        "\n",
        "        # conv_block takes in_channels, out_channels, kernel_size, stride, padding\n",
        "        # Inception block takes out1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool\n",
        "\n",
        "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.maxpool1 = nn.MaxPool2d(3, 2, 1)\n",
        "        self.conv2 = conv_block(64, 192, kernel_size=3, stride=1, padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(3, 2, 1)\n",
        "        self.inception3a = Inception_block(192, 64, 96, 128, 16, 32, 32)\n",
        "        self.inception3b = Inception_block(256, 128, 128, 192, 32, 96, 64)\n",
        "        self.maxpool3 = nn.MaxPool2d(3, 2, 1)\n",
        "        self.inception4a = Inception_block(480, 192, 96, 208, 16, 48, 64)\n",
        "\n",
        "        # auxiliary classifier\n",
        "\n",
        "        self.inception4b = Inception_block(512, 160, 112, 224, 24, 64, 64)\n",
        "        self.inception4c = Inception_block(512, 128, 128, 256, 24, 64, 64)\n",
        "        self.inception4d = Inception_block(512, 112, 144, 288, 32, 64, 64)\n",
        "\n",
        "        # auxiliary classifier\n",
        "\n",
        "        self.inception4e = Inception_block(528, 256, 160, 320, 32, 128, 128)\n",
        "        self.maxpool4 = nn.MaxPool2d(3, 2, 1)\n",
        "        self.inception5a = Inception_block(832, 256, 160, 320, 32, 128, 128)\n",
        "        self.inception5b = Inception_block(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(7, 1)\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.fc1 = nn.Linear(1024, num_classes)\n",
        "\n",
        "        if self.aux_logits:\n",
        "            self.aux1 = InceptionAux(512, num_classes)\n",
        "            self.aux2 = InceptionAux(528, num_classes)\n",
        "        else:\n",
        "            self.aux1 = self.aux2 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "        x = self.maxpool3(x)\n",
        "        x = self.inception4a(x)\n",
        "\n",
        "        if self.aux_logits and self.training:\n",
        "            aux1 = self.aux1(x)\n",
        "\n",
        "        x = self.inception4b(x)\n",
        "        x = self.inception4c(x)\n",
        "        x = self.inception4d(x)\n",
        "\n",
        "        if self.aux_logits and self.training:\n",
        "            aux2 = self.aux2(x)\n",
        "\n",
        "        x = self.inception4e(x)\n",
        "        x = self.maxpool4(x)\n",
        "        x = self.inception5a(x)\n",
        "        x = self.inception5b(x)\n",
        "        x = self.avgpool(x)\n",
        "\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        if self.aux_logits and self.training:\n",
        "            return x, aux1, aux2\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "class conv_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(conv_block, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, **kwargs),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_layer(x)\n",
        "\n",
        "\n",
        "class Inception_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n",
        "        super(Inception_block, self).__init__()\n",
        "\n",
        "        self.branch1 = conv_block(in_channels, out_1x1, kernel_size=1)\n",
        "\n",
        "        self.branch2 = nn.Sequential(\n",
        "            conv_block(in_channels, red_3x3, kernel_size=1),\n",
        "            conv_block(red_3x3, out_3x3, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.branch3 = nn.Sequential(\n",
        "            conv_block(in_channels, red_5x5, kernel_size=1),\n",
        "            conv_block(red_5x5, out_5x5, kernel_size=5, padding=2),\n",
        "        )\n",
        "\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            conv_block(in_channels, out_1x1pool, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 0차원은 batch이므로 1차원인 filter 수를 기준으로 각 branch의 출력값을 묶어줍니다.\n",
        "        x = torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1)\n",
        "        return x\n",
        "\n",
        "# auxiliary classifier의 loss는 0.3이 곱해지고, 최종 loss에 추가합니다. 정규화 효과가 있습니다.\n",
        "class InceptionAux(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(InceptionAux, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=5, stride=3),\n",
        "            conv_block(in_channels, 128, kernel_size=1),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GylRBTQAb1sD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GoogLeNet(aux_logits= True, num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "t8KmxO2kb3sf"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYh9SsRJaVb9",
        "outputId": "e45369fa-aced-416c-e6aa-06e357c6ee84"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,472\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "        conv_block-4         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-5           [-1, 64, 56, 56]               0\n",
            "            Conv2d-6          [-1, 192, 56, 56]         110,784\n",
            "       BatchNorm2d-7          [-1, 192, 56, 56]             384\n",
            "              ReLU-8          [-1, 192, 56, 56]               0\n",
            "        conv_block-9          [-1, 192, 56, 56]               0\n",
            "        MaxPool2d-10          [-1, 192, 28, 28]               0\n",
            "           Conv2d-11           [-1, 64, 28, 28]          12,352\n",
            "      BatchNorm2d-12           [-1, 64, 28, 28]             128\n",
            "             ReLU-13           [-1, 64, 28, 28]               0\n",
            "       conv_block-14           [-1, 64, 28, 28]               0\n",
            "           Conv2d-15           [-1, 96, 28, 28]          18,528\n",
            "      BatchNorm2d-16           [-1, 96, 28, 28]             192\n",
            "             ReLU-17           [-1, 96, 28, 28]               0\n",
            "       conv_block-18           [-1, 96, 28, 28]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]         110,720\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "       conv_block-22          [-1, 128, 28, 28]               0\n",
            "           Conv2d-23           [-1, 16, 28, 28]           3,088\n",
            "      BatchNorm2d-24           [-1, 16, 28, 28]              32\n",
            "             ReLU-25           [-1, 16, 28, 28]               0\n",
            "       conv_block-26           [-1, 16, 28, 28]               0\n",
            "           Conv2d-27           [-1, 32, 28, 28]          12,832\n",
            "      BatchNorm2d-28           [-1, 32, 28, 28]              64\n",
            "             ReLU-29           [-1, 32, 28, 28]               0\n",
            "       conv_block-30           [-1, 32, 28, 28]               0\n",
            "        MaxPool2d-31          [-1, 192, 28, 28]               0\n",
            "           Conv2d-32           [-1, 32, 28, 28]           6,176\n",
            "      BatchNorm2d-33           [-1, 32, 28, 28]              64\n",
            "             ReLU-34           [-1, 32, 28, 28]               0\n",
            "       conv_block-35           [-1, 32, 28, 28]               0\n",
            "  Inception_block-36          [-1, 256, 28, 28]               0\n",
            "           Conv2d-37          [-1, 128, 28, 28]          32,896\n",
            "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
            "             ReLU-39          [-1, 128, 28, 28]               0\n",
            "       conv_block-40          [-1, 128, 28, 28]               0\n",
            "           Conv2d-41          [-1, 128, 28, 28]          32,896\n",
            "      BatchNorm2d-42          [-1, 128, 28, 28]             256\n",
            "             ReLU-43          [-1, 128, 28, 28]               0\n",
            "       conv_block-44          [-1, 128, 28, 28]               0\n",
            "           Conv2d-45          [-1, 192, 28, 28]         221,376\n",
            "      BatchNorm2d-46          [-1, 192, 28, 28]             384\n",
            "             ReLU-47          [-1, 192, 28, 28]               0\n",
            "       conv_block-48          [-1, 192, 28, 28]               0\n",
            "           Conv2d-49           [-1, 32, 28, 28]           8,224\n",
            "      BatchNorm2d-50           [-1, 32, 28, 28]              64\n",
            "             ReLU-51           [-1, 32, 28, 28]               0\n",
            "       conv_block-52           [-1, 32, 28, 28]               0\n",
            "           Conv2d-53           [-1, 96, 28, 28]          76,896\n",
            "      BatchNorm2d-54           [-1, 96, 28, 28]             192\n",
            "             ReLU-55           [-1, 96, 28, 28]               0\n",
            "       conv_block-56           [-1, 96, 28, 28]               0\n",
            "        MaxPool2d-57          [-1, 256, 28, 28]               0\n",
            "           Conv2d-58           [-1, 64, 28, 28]          16,448\n",
            "      BatchNorm2d-59           [-1, 64, 28, 28]             128\n",
            "             ReLU-60           [-1, 64, 28, 28]               0\n",
            "       conv_block-61           [-1, 64, 28, 28]               0\n",
            "  Inception_block-62          [-1, 480, 28, 28]               0\n",
            "        MaxPool2d-63          [-1, 480, 14, 14]               0\n",
            "           Conv2d-64          [-1, 192, 14, 14]          92,352\n",
            "      BatchNorm2d-65          [-1, 192, 14, 14]             384\n",
            "             ReLU-66          [-1, 192, 14, 14]               0\n",
            "       conv_block-67          [-1, 192, 14, 14]               0\n",
            "           Conv2d-68           [-1, 96, 14, 14]          46,176\n",
            "      BatchNorm2d-69           [-1, 96, 14, 14]             192\n",
            "             ReLU-70           [-1, 96, 14, 14]               0\n",
            "       conv_block-71           [-1, 96, 14, 14]               0\n",
            "           Conv2d-72          [-1, 208, 14, 14]         179,920\n",
            "      BatchNorm2d-73          [-1, 208, 14, 14]             416\n",
            "             ReLU-74          [-1, 208, 14, 14]               0\n",
            "       conv_block-75          [-1, 208, 14, 14]               0\n",
            "           Conv2d-76           [-1, 16, 14, 14]           7,696\n",
            "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
            "             ReLU-78           [-1, 16, 14, 14]               0\n",
            "       conv_block-79           [-1, 16, 14, 14]               0\n",
            "           Conv2d-80           [-1, 48, 14, 14]          19,248\n",
            "      BatchNorm2d-81           [-1, 48, 14, 14]              96\n",
            "             ReLU-82           [-1, 48, 14, 14]               0\n",
            "       conv_block-83           [-1, 48, 14, 14]               0\n",
            "        MaxPool2d-84          [-1, 480, 14, 14]               0\n",
            "           Conv2d-85           [-1, 64, 14, 14]          30,784\n",
            "      BatchNorm2d-86           [-1, 64, 14, 14]             128\n",
            "             ReLU-87           [-1, 64, 14, 14]               0\n",
            "       conv_block-88           [-1, 64, 14, 14]               0\n",
            "  Inception_block-89          [-1, 512, 14, 14]               0\n",
            "           Conv2d-90          [-1, 160, 14, 14]          82,080\n",
            "      BatchNorm2d-91          [-1, 160, 14, 14]             320\n",
            "             ReLU-92          [-1, 160, 14, 14]               0\n",
            "       conv_block-93          [-1, 160, 14, 14]               0\n",
            "           Conv2d-94          [-1, 112, 14, 14]          57,456\n",
            "      BatchNorm2d-95          [-1, 112, 14, 14]             224\n",
            "             ReLU-96          [-1, 112, 14, 14]               0\n",
            "       conv_block-97          [-1, 112, 14, 14]               0\n",
            "           Conv2d-98          [-1, 224, 14, 14]         226,016\n",
            "      BatchNorm2d-99          [-1, 224, 14, 14]             448\n",
            "            ReLU-100          [-1, 224, 14, 14]               0\n",
            "      conv_block-101          [-1, 224, 14, 14]               0\n",
            "          Conv2d-102           [-1, 24, 14, 14]          12,312\n",
            "     BatchNorm2d-103           [-1, 24, 14, 14]              48\n",
            "            ReLU-104           [-1, 24, 14, 14]               0\n",
            "      conv_block-105           [-1, 24, 14, 14]               0\n",
            "          Conv2d-106           [-1, 64, 14, 14]          38,464\n",
            "     BatchNorm2d-107           [-1, 64, 14, 14]             128\n",
            "            ReLU-108           [-1, 64, 14, 14]               0\n",
            "      conv_block-109           [-1, 64, 14, 14]               0\n",
            "       MaxPool2d-110          [-1, 512, 14, 14]               0\n",
            "          Conv2d-111           [-1, 64, 14, 14]          32,832\n",
            "     BatchNorm2d-112           [-1, 64, 14, 14]             128\n",
            "            ReLU-113           [-1, 64, 14, 14]               0\n",
            "      conv_block-114           [-1, 64, 14, 14]               0\n",
            " Inception_block-115          [-1, 512, 14, 14]               0\n",
            "          Conv2d-116          [-1, 128, 14, 14]          65,664\n",
            "     BatchNorm2d-117          [-1, 128, 14, 14]             256\n",
            "            ReLU-118          [-1, 128, 14, 14]               0\n",
            "      conv_block-119          [-1, 128, 14, 14]               0\n",
            "          Conv2d-120          [-1, 128, 14, 14]          65,664\n",
            "     BatchNorm2d-121          [-1, 128, 14, 14]             256\n",
            "            ReLU-122          [-1, 128, 14, 14]               0\n",
            "      conv_block-123          [-1, 128, 14, 14]               0\n",
            "          Conv2d-124          [-1, 256, 14, 14]         295,168\n",
            "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
            "            ReLU-126          [-1, 256, 14, 14]               0\n",
            "      conv_block-127          [-1, 256, 14, 14]               0\n",
            "          Conv2d-128           [-1, 24, 14, 14]          12,312\n",
            "     BatchNorm2d-129           [-1, 24, 14, 14]              48\n",
            "            ReLU-130           [-1, 24, 14, 14]               0\n",
            "      conv_block-131           [-1, 24, 14, 14]               0\n",
            "          Conv2d-132           [-1, 64, 14, 14]          38,464\n",
            "     BatchNorm2d-133           [-1, 64, 14, 14]             128\n",
            "            ReLU-134           [-1, 64, 14, 14]               0\n",
            "      conv_block-135           [-1, 64, 14, 14]               0\n",
            "       MaxPool2d-136          [-1, 512, 14, 14]               0\n",
            "          Conv2d-137           [-1, 64, 14, 14]          32,832\n",
            "     BatchNorm2d-138           [-1, 64, 14, 14]             128\n",
            "            ReLU-139           [-1, 64, 14, 14]               0\n",
            "      conv_block-140           [-1, 64, 14, 14]               0\n",
            " Inception_block-141          [-1, 512, 14, 14]               0\n",
            "          Conv2d-142          [-1, 112, 14, 14]          57,456\n",
            "     BatchNorm2d-143          [-1, 112, 14, 14]             224\n",
            "            ReLU-144          [-1, 112, 14, 14]               0\n",
            "      conv_block-145          [-1, 112, 14, 14]               0\n",
            "          Conv2d-146          [-1, 144, 14, 14]          73,872\n",
            "     BatchNorm2d-147          [-1, 144, 14, 14]             288\n",
            "            ReLU-148          [-1, 144, 14, 14]               0\n",
            "      conv_block-149          [-1, 144, 14, 14]               0\n",
            "          Conv2d-150          [-1, 288, 14, 14]         373,536\n",
            "     BatchNorm2d-151          [-1, 288, 14, 14]             576\n",
            "            ReLU-152          [-1, 288, 14, 14]               0\n",
            "      conv_block-153          [-1, 288, 14, 14]               0\n",
            "          Conv2d-154           [-1, 32, 14, 14]          16,416\n",
            "     BatchNorm2d-155           [-1, 32, 14, 14]              64\n",
            "            ReLU-156           [-1, 32, 14, 14]               0\n",
            "      conv_block-157           [-1, 32, 14, 14]               0\n",
            "          Conv2d-158           [-1, 64, 14, 14]          51,264\n",
            "     BatchNorm2d-159           [-1, 64, 14, 14]             128\n",
            "            ReLU-160           [-1, 64, 14, 14]               0\n",
            "      conv_block-161           [-1, 64, 14, 14]               0\n",
            "       MaxPool2d-162          [-1, 512, 14, 14]               0\n",
            "          Conv2d-163           [-1, 64, 14, 14]          32,832\n",
            "     BatchNorm2d-164           [-1, 64, 14, 14]             128\n",
            "            ReLU-165           [-1, 64, 14, 14]               0\n",
            "      conv_block-166           [-1, 64, 14, 14]               0\n",
            " Inception_block-167          [-1, 528, 14, 14]               0\n",
            "          Conv2d-168          [-1, 256, 14, 14]         135,424\n",
            "     BatchNorm2d-169          [-1, 256, 14, 14]             512\n",
            "            ReLU-170          [-1, 256, 14, 14]               0\n",
            "      conv_block-171          [-1, 256, 14, 14]               0\n",
            "          Conv2d-172          [-1, 160, 14, 14]          84,640\n",
            "     BatchNorm2d-173          [-1, 160, 14, 14]             320\n",
            "            ReLU-174          [-1, 160, 14, 14]               0\n",
            "      conv_block-175          [-1, 160, 14, 14]               0\n",
            "          Conv2d-176          [-1, 320, 14, 14]         461,120\n",
            "     BatchNorm2d-177          [-1, 320, 14, 14]             640\n",
            "            ReLU-178          [-1, 320, 14, 14]               0\n",
            "      conv_block-179          [-1, 320, 14, 14]               0\n",
            "          Conv2d-180           [-1, 32, 14, 14]          16,928\n",
            "     BatchNorm2d-181           [-1, 32, 14, 14]              64\n",
            "            ReLU-182           [-1, 32, 14, 14]               0\n",
            "      conv_block-183           [-1, 32, 14, 14]               0\n",
            "          Conv2d-184          [-1, 128, 14, 14]         102,528\n",
            "     BatchNorm2d-185          [-1, 128, 14, 14]             256\n",
            "            ReLU-186          [-1, 128, 14, 14]               0\n",
            "      conv_block-187          [-1, 128, 14, 14]               0\n",
            "       MaxPool2d-188          [-1, 528, 14, 14]               0\n",
            "          Conv2d-189          [-1, 128, 14, 14]          67,712\n",
            "     BatchNorm2d-190          [-1, 128, 14, 14]             256\n",
            "            ReLU-191          [-1, 128, 14, 14]               0\n",
            "      conv_block-192          [-1, 128, 14, 14]               0\n",
            " Inception_block-193          [-1, 832, 14, 14]               0\n",
            "       MaxPool2d-194            [-1, 832, 7, 7]               0\n",
            "          Conv2d-195            [-1, 256, 7, 7]         213,248\n",
            "     BatchNorm2d-196            [-1, 256, 7, 7]             512\n",
            "            ReLU-197            [-1, 256, 7, 7]               0\n",
            "      conv_block-198            [-1, 256, 7, 7]               0\n",
            "          Conv2d-199            [-1, 160, 7, 7]         133,280\n",
            "     BatchNorm2d-200            [-1, 160, 7, 7]             320\n",
            "            ReLU-201            [-1, 160, 7, 7]               0\n",
            "      conv_block-202            [-1, 160, 7, 7]               0\n",
            "          Conv2d-203            [-1, 320, 7, 7]         461,120\n",
            "     BatchNorm2d-204            [-1, 320, 7, 7]             640\n",
            "            ReLU-205            [-1, 320, 7, 7]               0\n",
            "      conv_block-206            [-1, 320, 7, 7]               0\n",
            "          Conv2d-207             [-1, 32, 7, 7]          26,656\n",
            "     BatchNorm2d-208             [-1, 32, 7, 7]              64\n",
            "            ReLU-209             [-1, 32, 7, 7]               0\n",
            "      conv_block-210             [-1, 32, 7, 7]               0\n",
            "          Conv2d-211            [-1, 128, 7, 7]         102,528\n",
            "     BatchNorm2d-212            [-1, 128, 7, 7]             256\n",
            "            ReLU-213            [-1, 128, 7, 7]               0\n",
            "      conv_block-214            [-1, 128, 7, 7]               0\n",
            "       MaxPool2d-215            [-1, 832, 7, 7]               0\n",
            "          Conv2d-216            [-1, 128, 7, 7]         106,624\n",
            "     BatchNorm2d-217            [-1, 128, 7, 7]             256\n",
            "            ReLU-218            [-1, 128, 7, 7]               0\n",
            "      conv_block-219            [-1, 128, 7, 7]               0\n",
            " Inception_block-220            [-1, 832, 7, 7]               0\n",
            "          Conv2d-221            [-1, 384, 7, 7]         319,872\n",
            "     BatchNorm2d-222            [-1, 384, 7, 7]             768\n",
            "            ReLU-223            [-1, 384, 7, 7]               0\n",
            "      conv_block-224            [-1, 384, 7, 7]               0\n",
            "          Conv2d-225            [-1, 192, 7, 7]         159,936\n",
            "     BatchNorm2d-226            [-1, 192, 7, 7]             384\n",
            "            ReLU-227            [-1, 192, 7, 7]               0\n",
            "      conv_block-228            [-1, 192, 7, 7]               0\n",
            "          Conv2d-229            [-1, 384, 7, 7]         663,936\n",
            "     BatchNorm2d-230            [-1, 384, 7, 7]             768\n",
            "            ReLU-231            [-1, 384, 7, 7]               0\n",
            "      conv_block-232            [-1, 384, 7, 7]               0\n",
            "          Conv2d-233             [-1, 48, 7, 7]          39,984\n",
            "     BatchNorm2d-234             [-1, 48, 7, 7]              96\n",
            "            ReLU-235             [-1, 48, 7, 7]               0\n",
            "      conv_block-236             [-1, 48, 7, 7]               0\n",
            "          Conv2d-237            [-1, 128, 7, 7]         153,728\n",
            "     BatchNorm2d-238            [-1, 128, 7, 7]             256\n",
            "            ReLU-239            [-1, 128, 7, 7]               0\n",
            "      conv_block-240            [-1, 128, 7, 7]               0\n",
            "       MaxPool2d-241            [-1, 832, 7, 7]               0\n",
            "          Conv2d-242            [-1, 128, 7, 7]         106,624\n",
            "     BatchNorm2d-243            [-1, 128, 7, 7]             256\n",
            "            ReLU-244            [-1, 128, 7, 7]               0\n",
            "      conv_block-245            [-1, 128, 7, 7]               0\n",
            " Inception_block-246           [-1, 1024, 7, 7]               0\n",
            "       AvgPool2d-247           [-1, 1024, 1, 1]               0\n",
            "         Dropout-248                 [-1, 1024]               0\n",
            "          Linear-249                   [-1, 10]          10,250\n",
            "================================================================\n",
            "Total params: 5,994,074\n",
            "Trainable params: 5,994,074\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 112.59\n",
            "Params size (MB): 22.87\n",
            "Estimated Total Size (MB): 136.03\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "VezQ9rOpd6oy"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "UvZ9hSv8rr4P"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "YsCpzmGJd8ke"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        if pred.shape[0] == 3:\n",
        "            output, aux1, aux2 = pred\n",
        "            output_loss = loss_fn(output, y)\n",
        "            aux1_loss = loss_fn(aux1, y)\n",
        "            aux2_loss = loss_fn(aux2, y)\n",
        "\n",
        "            loss = output_loss + 0.3*(aux1_loss + aux2_loss)\n",
        "\n",
        "        else:\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "uhO55RXtd93_"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "97Zp3x37d_TP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a2db56-1cab-4f8b-90ed-c5cb59636547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.556319  [   64/50000]\n",
            "loss: 0.353972  [ 6464/50000]\n",
            "loss: 0.330058  [12864/50000]\n",
            "loss: 0.518296  [19264/50000]\n",
            "loss: 0.295406  [25664/50000]\n",
            "loss: 0.910707  [32064/50000]\n",
            "loss: 0.570953  [38464/50000]\n",
            "loss: 0.724823  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 80.1%, Avg loss: 0.581571 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.367271  [   64/50000]\n",
            "loss: 0.357621  [ 6464/50000]\n",
            "loss: 0.274603  [12864/50000]\n",
            "loss: 0.512808  [19264/50000]\n",
            "loss: 0.359156  [25664/50000]\n",
            "loss: 0.730275  [32064/50000]\n",
            "loss: 0.545609  [38464/50000]\n",
            "loss: 0.476580  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 80.9%, Avg loss: 0.574363 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.290505  [   64/50000]\n",
            "loss: 0.321871  [ 6464/50000]\n",
            "loss: 0.214357  [12864/50000]\n",
            "loss: 0.311389  [19264/50000]\n",
            "loss: 0.180077  [25664/50000]\n",
            "loss: 0.407627  [32064/50000]\n",
            "loss: 0.202369  [38464/50000]\n",
            "loss: 0.386298  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 81.4%, Avg loss: 0.598875 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.244406  [   64/50000]\n",
            "loss: 0.332691  [ 6464/50000]\n",
            "loss: 0.194279  [12864/50000]\n",
            "loss: 0.128858  [19264/50000]\n",
            "loss: 0.124070  [25664/50000]\n",
            "loss: 0.491661  [32064/50000]\n",
            "loss: 0.166129  [38464/50000]\n",
            "loss: 0.253614  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 83.4%, Avg loss: 0.554371 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.189019  [   64/50000]\n",
            "loss: 0.170606  [ 6464/50000]\n",
            "loss: 0.083584  [12864/50000]\n",
            "loss: 0.247657  [19264/50000]\n",
            "loss: 0.107871  [25664/50000]\n",
            "loss: 0.218749  [32064/50000]\n",
            "loss: 0.144526  [38464/50000]\n",
            "loss: 0.187880  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 83.9%, Avg loss: 0.587198 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.090956  [   64/50000]\n",
            "loss: 0.150926  [ 6464/50000]\n",
            "loss: 0.069447  [12864/50000]\n",
            "loss: 0.177628  [19264/50000]\n",
            "loss: 0.078360  [25664/50000]\n",
            "loss: 0.146594  [32064/50000]\n",
            "loss: 0.122155  [38464/50000]\n",
            "loss: 0.171579  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 81.3%, Avg loss: 0.785482 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.127766  [   64/50000]\n",
            "loss: 0.224138  [ 6464/50000]\n",
            "loss: 0.031222  [12864/50000]\n",
            "loss: 0.128922  [19264/50000]\n",
            "loss: 0.161864  [25664/50000]\n",
            "loss: 0.288244  [32064/50000]\n",
            "loss: 0.153319  [38464/50000]\n",
            "loss: 0.119891  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 83.5%, Avg loss: 0.700414 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.071001  [   64/50000]\n",
            "loss: 0.154124  [ 6464/50000]\n",
            "loss: 0.037031  [12864/50000]\n",
            "loss: 0.140726  [19264/50000]\n",
            "loss: 0.114841  [25664/50000]\n",
            "loss: 0.188420  [32064/50000]\n",
            "loss: 0.040845  [38464/50000]\n",
            "loss: 0.075536  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 84.1%, Avg loss: 0.695892 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.120673  [   64/50000]\n",
            "loss: 0.108027  [ 6464/50000]\n",
            "loss: 0.101903  [12864/50000]\n",
            "loss: 0.398689  [19264/50000]\n",
            "loss: 0.083821  [25664/50000]\n",
            "loss: 0.188982  [32064/50000]\n",
            "loss: 0.197347  [38464/50000]\n",
            "loss: 0.146617  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 85.3%, Avg loss: 0.639444 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.028005  [   64/50000]\n",
            "loss: 0.040169  [ 6464/50000]\n",
            "loss: 0.008277  [12864/50000]\n",
            "loss: 0.049620  [19264/50000]\n",
            "loss: 0.046595  [25664/50000]\n",
            "loss: 0.071825  [32064/50000]\n",
            "loss: 0.202751  [38464/50000]\n",
            "loss: 0.063777  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 85.5%, Avg loss: 0.691961 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyN9a7T0WYzHkPpdILyH6Sjb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}